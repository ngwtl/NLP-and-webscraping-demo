{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Subreddit Classification\n",
    "---\n",
    "Project notebook organisation:<br>\n",
    "[1 - Webscraping and data acquisition](./1_webscraping_and_data_acquisition.ipynb)<br>\n",
    "[2 - Preprocessing of data](./2_preprocessing.ipynb)<br>\n",
    "[3 - Exploratory data analysis](./3_eda.ipynb)<br>\n",
    "**4 - Model Tuning and Insights** (current notebook)<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_classif, chi2\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import regex as re\n",
    "\n",
    "sns.set_style('ticks')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will be using the cleaned data from the previous notebook to train and evaluate a model that classify new posts to either r/Androidquestions or r/iphonehelp. I tried different combinations of predictors as well as feature selection techniques for multiple different models - Support Vector Machines, Binomial Naive Bayes, Multinomial Naive Bayes and Logistic Regression. The models were evaluated based on their accuracy scores on unseen validation data, before the best performing model was used to score the test data.\n",
    "\n",
    "\n",
    "### Contents\n",
    "1. [Data preparation](#Data-preparation)\n",
    "2. [Modelling approach](#Modelling-approach)\n",
    "3. [Preprocessing](#Preprocessing)\n",
    "4. [Production model selection](#Production-model-selection)\n",
    "5. [Conclusion and recommendations](#Conclusion-and-recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe contains 1982 rows and 12 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1982, 12)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>date_created</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_all</th>\n",
       "      <th>comment_len</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apps wont load using WIFI</td>\n",
       "      <td>hcerke</td>\n",
       "      <td>2020-06-20 03:16:44</td>\n",
       "      <td>Hello, my apps are not loading content when i ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is there any way to select photos from differe...</td>\n",
       "      <td>hcekrf</td>\n",
       "      <td>2020-06-20 03:03:21</td>\n",
       "      <td>I'm using a Samsung Note 8 and just the stock ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chat Features No Longer Working</td>\n",
       "      <td>hcefkx</td>\n",
       "      <td>2020-06-20 02:52:45</td>\n",
       "      <td>So I use a Samsung Galaxy S10+ on Verizon and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Try the following, turn off WiFi and turn on m...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An app updated itself in a weird way</td>\n",
       "      <td>hcefi8</td>\n",
       "      <td>2020-06-20 02:52:36</td>\n",
       "      <td>I was using the Twitch app on android, and som...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Google Photos] Is it possible to remove pictu...</td>\n",
       "      <td>hcdh2b</td>\n",
       "      <td>2020-06-20 01:43:45</td>\n",
       "      <td>I joined a public album that is just a big mix...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      id  \\\n",
       "0                          Apps wont load using WIFI  hcerke   \n",
       "1  Is there any way to select photos from differe...  hcekrf   \n",
       "2                    Chat Features No Longer Working  hcefkx   \n",
       "3               An app updated itself in a weird way  hcefi8   \n",
       "4  [Google Photos] Is it possible to remove pictu...  hcdh2b   \n",
       "\n",
       "          date_created                                               text  \\\n",
       "0  2020-06-20 03:16:44  Hello, my apps are not loading content when i ...   \n",
       "1  2020-06-20 03:03:21  I'm using a Samsung Note 8 and just the stock ...   \n",
       "2  2020-06-20 02:52:45  So I use a Samsung Galaxy S10+ on Verizon and ...   \n",
       "3  2020-06-20 02:52:36  I was using the Twitch app on android, and som...   \n",
       "4  2020-06-20 01:43:45  I joined a public album that is just a big mix...   \n",
       "\n",
       "   score  upvote_ratio  comment_count  \\\n",
       "0      1           1.0            0.0   \n",
       "1      1           1.0            0.0   \n",
       "2      1           1.0            1.0   \n",
       "3      1           1.0            0.0   \n",
       "4      1           1.0            0.0   \n",
       "\n",
       "                                         comment_all  comment_len  title_len  \\\n",
       "0                                         notexthere          0.0          5   \n",
       "1                                         notexthere          0.0         14   \n",
       "2  Try the following, turn off WiFi and turn on m...         40.0          5   \n",
       "3                                         notexthere          0.0          8   \n",
       "4                                         notexthere          0.0         17   \n",
       "\n",
       "   text_len  subreddit  \n",
       "0        27          1  \n",
       "1        60          1  \n",
       "2        75          1  \n",
       "3        90          1  \n",
       "4        77          1  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "id               0\n",
       "date_created     0\n",
       "text             6\n",
       "score            0\n",
       "upvote_ratio     0\n",
       "comment_count    0\n",
       "comment_all      2\n",
       "comment_len      0\n",
       "title_len        0\n",
       "text_len         0\n",
       "subreddit        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in the previous notebook, despite having already run a pre-processing function and having checked for null values\n",
    "# in the previous notebook, a number of null values still appear in the `text` and `comment_all` fields. This function\n",
    "# rectifies that.\n",
    "\n",
    "\n",
    "def nullfiller(df):   \n",
    "   \n",
    "    # fill in null values for posts with no comments\n",
    "    df['comment_all'].fillna('notexthere', inplace = True)\n",
    "    df['text'].fillna('notexthere', inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>date_created</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_all</th>\n",
       "      <th>comment_len</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apps wont load using WIFI</td>\n",
       "      <td>hcerke</td>\n",
       "      <td>2020-06-20 03:16:44</td>\n",
       "      <td>Hello, my apps are not loading content when i ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is there any way to select photos from differe...</td>\n",
       "      <td>hcekrf</td>\n",
       "      <td>2020-06-20 03:03:21</td>\n",
       "      <td>I'm using a Samsung Note 8 and just the stock ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chat Features No Longer Working</td>\n",
       "      <td>hcefkx</td>\n",
       "      <td>2020-06-20 02:52:45</td>\n",
       "      <td>So I use a Samsung Galaxy S10+ on Verizon and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Try the following, turn off WiFi and turn on m...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An app updated itself in a weird way</td>\n",
       "      <td>hcefi8</td>\n",
       "      <td>2020-06-20 02:52:36</td>\n",
       "      <td>I was using the Twitch app on android, and som...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Google Photos] Is it possible to remove pictu...</td>\n",
       "      <td>hcdh2b</td>\n",
       "      <td>2020-06-20 01:43:45</td>\n",
       "      <td>I joined a public album that is just a big mix...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Logged in &amp; out of iCloud and now passwords ar...</td>\n",
       "      <td>g8e88l</td>\n",
       "      <td>2020-04-26 13:19:55</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>Need help transferring photos and videos from ...</td>\n",
       "      <td>g8e0ke</td>\n",
       "      <td>2020-04-26 13:05:16</td>\n",
       "      <td>As the title explains. I'v tried using the win...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>reinstall the apple drivers\\n\\n  Settings &gt; Ph...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>12</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>Can get past \"Welcome to Mail\" in Mail iOS 13.4.1</td>\n",
       "      <td>g8cbyw</td>\n",
       "      <td>2020-04-26 10:47:19</td>\n",
       "      <td>notexthere</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>screw that app, someone can literally hack you...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Can you replace an iPhone 5S and 5SE battery?</td>\n",
       "      <td>g8bspe</td>\n",
       "      <td>2020-04-26 09:58:35</td>\n",
       "      <td>My 5S and 5SE are draining battery quicker tha...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Note, it’s the SE, not 5SE ;-). Of course you ...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>iPhone 6S keeps restarting</td>\n",
       "      <td>g88e7j</td>\n",
       "      <td>2020-04-26 04:53:04</td>\n",
       "      <td>I dropped my phone with a case on. No physical...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Backups are most useful *before* they’re neede...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1982 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title      id  \\\n",
       "0                            Apps wont load using WIFI  hcerke   \n",
       "1    Is there any way to select photos from differe...  hcekrf   \n",
       "2                      Chat Features No Longer Working  hcefkx   \n",
       "3                 An app updated itself in a weird way  hcefi8   \n",
       "4    [Google Photos] Is it possible to remove pictu...  hcdh2b   \n",
       "..                                                 ...     ...   \n",
       "987  Logged in & out of iCloud and now passwords ar...  g8e88l   \n",
       "988  Need help transferring photos and videos from ...  g8e0ke   \n",
       "989  Can get past \"Welcome to Mail\" in Mail iOS 13.4.1  g8cbyw   \n",
       "990      Can you replace an iPhone 5S and 5SE battery?  g8bspe   \n",
       "991                         iPhone 6S keeps restarting  g88e7j   \n",
       "\n",
       "            date_created                                               text  \\\n",
       "0    2020-06-20 03:16:44  Hello, my apps are not loading content when i ...   \n",
       "1    2020-06-20 03:03:21  I'm using a Samsung Note 8 and just the stock ...   \n",
       "2    2020-06-20 02:52:45  So I use a Samsung Galaxy S10+ on Verizon and ...   \n",
       "3    2020-06-20 02:52:36  I was using the Twitch app on android, and som...   \n",
       "4    2020-06-20 01:43:45  I joined a public album that is just a big mix...   \n",
       "..                   ...                                                ...   \n",
       "987  2020-04-26 13:19:55                                         notexthere   \n",
       "988  2020-04-26 13:05:16  As the title explains. I'v tried using the win...   \n",
       "989  2020-04-26 10:47:19                                         notexthere   \n",
       "990  2020-04-26 09:58:35  My 5S and 5SE are draining battery quicker tha...   \n",
       "991  2020-04-26 04:53:04  I dropped my phone with a case on. No physical...   \n",
       "\n",
       "     score  upvote_ratio  comment_count  \\\n",
       "0        1           1.0            0.0   \n",
       "1        1           1.0            0.0   \n",
       "2        1           1.0            1.0   \n",
       "3        1           1.0            0.0   \n",
       "4        1           1.0            0.0   \n",
       "..     ...           ...            ...   \n",
       "987      0           0.5            0.0   \n",
       "988      1           1.0            4.0   \n",
       "989      4           1.0            1.0   \n",
       "990      2           1.0            3.0   \n",
       "991      1           1.0            2.0   \n",
       "\n",
       "                                           comment_all  comment_len  \\\n",
       "0                                           notexthere          0.0   \n",
       "1                                           notexthere          0.0   \n",
       "2    Try the following, turn off WiFi and turn on m...         40.0   \n",
       "3                                           notexthere          0.0   \n",
       "4                                           notexthere          0.0   \n",
       "..                                                 ...          ...   \n",
       "987                                         notexthere          0.0   \n",
       "988  reinstall the apple drivers\\n\\n  Settings > Ph...         94.0   \n",
       "989  screw that app, someone can literally hack you...         33.0   \n",
       "990  Note, it’s the SE, not 5SE ;-). Of course you ...         37.0   \n",
       "991  Backups are most useful *before* they’re neede...         89.0   \n",
       "\n",
       "     title_len  text_len  subreddit  \n",
       "0            5        27          1  \n",
       "1           14        60          1  \n",
       "2            5        75          1  \n",
       "3            8        90          1  \n",
       "4           17        77          1  \n",
       "..         ...       ...        ...  \n",
       "987         18         1          0  \n",
       "988         12        96          0  \n",
       "989         10         1          0  \n",
       "990          9        29          0  \n",
       "991          4        51          0  \n",
       "\n",
       "[1982 rows x 12 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nullfiller(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title               0\n",
       "id                  0\n",
       "date_created        0\n",
       "text                0\n",
       "score               0\n",
       "upvote_ratio        0\n",
       "comment_count       0\n",
       "comment_all         0\n",
       "comment_len         0\n",
       "title_len           0\n",
       "text_len            0\n",
       "subreddit           0\n",
       "tok_title           0\n",
       "tok_text            0\n",
       "tok_comments        0\n",
       "lem_tok_title       0\n",
       "lem_tok_text        0\n",
       "lem_tok_comments    0\n",
       "cleaned_title       0\n",
       "cleaned_text        0\n",
       "cleaned_comments    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains the rationale behind the preprocessing, production model selection and hyperparameter optimisation of the selected production model.\n",
    "\n",
    "There are 11 columns in the original dataframe (not counting the `subreddit` column, which contains the subreddit ID). Of these, the following contains text:\n",
    "- `title`\n",
    "- `text`\n",
    "- `comment_all`\n",
    "- `id`\n",
    "\n",
    "And the following contains numeric data:\n",
    "- `date_created`\n",
    "- `score`\n",
    "- `upvote_ratio`\n",
    "- `comment_count`\n",
    "- `comment_len`\n",
    "- `title_len`\n",
    "- `text_len`\n",
    "\n",
    "Among these, the columns most likely to be relevant in answering the problem statement (whether or not a new post can be correctly classified into r/Androidquestions or r/iphonehelp based on its content) are:\n",
    "- `title`\n",
    "- `text`\n",
    "- `comment_all`\n",
    "\n",
    "**Preprocessing**\n",
    "\n",
    "A workflow of 'tokenization - lemmatization - join text - clean punctation' is used in the preprocessing of data before model selection could begin. These steps were applied to the text strings `title`, `text` and `comment_all` in order to reduce the noise caused by capitalisations, past tense and punctuations. The result of this wokflow is then assigned new columns named `cleaned_title`, `cleaned_text` and `cleaned_comments` which will be used for modelling.\n",
    "\n",
    "In addition, stop words were defined using the default stopwords provided by `NLTK`. These were merged with the stop words identified in the previous notebook that intersected content from both posts. This complete list of stopwords is then used as a stopword corpus for vectorisation of the content.\n",
    "\n",
    "A baseline score is also extablished using the normalised counts of the the number of subreddit posts.\n",
    "\n",
    "**Production model selection**\n",
    "\n",
    "A staged approach is taken for model selection, firstly by identifying the ideal vectorisers (CountVectorizer or TfidVectorizer) and by identifying the most effective model (Support Vector Machines, Logistic Regression, Binomial Naive Bayes or Multinomial Naive Bayes) in the classification of data. I use accuracy as a metric for which combination is the most effective an good description of the effectiveness of the model as it describes exactly how many posts were identified correctly based on the entire corpus of words (number if True Positives and Negatives over the total sample). The most effective combination of vectoriser and classification model is selected using `GridSearchCV` without hyperparameter tuning and applied to the content located within `cleaned_title`, `cleaned_text` and `cleaned_comments`.\n",
    "\n",
    "Upon selection of the vectorizer and classification model, a second grid search is conducted this time to identify the optimised hyperparameters that can enhance the model's accuracy. Once again, this far more extensive grid search is applied to `cleaned_title`, `cleaned_text` and `cleaned_comments` in order to compare the effectiveness of using the individual content as a predictor for posts. \n",
    "\n",
    "Lastly, further feature engineering is then applied to the model, this time by combining all the content located in `cleaned_title`, `cleaned_text` and `cleaned_comments` into a single dataframe and fed into the optimised model in atempting to further improve the accuracy.\n",
    "\n",
    "This final production model is then tested on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df['tok_title'] = df.apply(lambda row: nltk.word_tokenize(row['title'].lower()), axis=1)\n",
    "df['tok_text'] = df.apply(lambda row: nltk.word_tokenize(row['text'].lower()), axis=1)\n",
    "df['tok_comments'] = df.apply(lambda row: nltk.word_tokenize(row['comment_all'].lower()), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['lem_tok_title'] = df['tok_title'].apply(lambda row: [lemmatizer.lemmatize(item) for item in row])\n",
    "df['lem_tok_text'] = df['tok_text'].apply(lambda row: [lemmatizer.lemmatize(item) for item in row])\n",
    "df['lem_tok_comments'] = df['tok_comments'].apply(lambda row: [lemmatizer.lemmatize(item) for item in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_title\"]= df[\"lem_tok_title\"].str.join(\" \")\n",
    "df[\"cleaned_text\"]= df[\"lem_tok_text\"].str.join(\" \")\n",
    "df[\"cleaned_comments\"]= df[\"lem_tok_text\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cleaned_title = df.cleaned_title.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "df.cleaned_text = df.cleaned_text.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "df.cleaned_comments = df.cleaned_comments.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default NLTK stopword list\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "# add additional stopwords\n",
    "additional_stopwords = {'don know', 'make sure', 'don think', 'sim card'\\\n",
    "                       , 'factory reset', 'new phone', 've tried', 'don want',\\\n",
    "                        'thanks advance', 'sim card', 'does know', 'doesn work', 'recovery mode',\\\n",
    "                         'days ago', 'lock screen', 'months ago', 'power button', 'old phone', \\\n",
    "                       'need help', 'factory reset', 'lock screen'}\n",
    "stop_words = stop_words.union(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      hello , my apps are not loading content when i...\n",
       "1      i 'm using a samsung note 8 and just the stock...\n",
       "2      so i use a samsung galaxy s10+ on verizon and ...\n",
       "3      i wa using the twitch app on android , and som...\n",
       "4      i joined a public album that is just a big mix...\n",
       "                             ...                        \n",
       "987                                           notexthere\n",
       "988    a the title explains . i ' v tried using the w...\n",
       "989                                           notexthere\n",
       "990    my 5 and 5se are draining battery quicker than...\n",
       "991    i dropped my phone with a case on . no physica...\n",
       "Name: cleaned_comments, Length: 1982, dtype: object"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1387, 20)\n",
      "(1387,)\n",
      "(595, 20)\n",
      "(595,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('subreddit', axis=1)\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                              y, \n",
    "                                              test_size = 0.3, \n",
    "                                              random_state = 42, \n",
    "                                              stratify = y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a binary classification problem, we can take the normalised value counts of the binary target feature in as the baseline score. The score of 50.1% is taken upon the assumption that this is the best score that we will get should our predictions be all '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.500505\n",
       "1    0.499495\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default NLTK stopword list\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "# add additional stopwords\n",
    "additional_stopwords = {'don', 'know', 'make', 'sure', 'think', 'sim', 'card'\\\n",
    "                       , 'factory', 'reset', 'new', 'phone', 've' 'tried', 'want',\\\n",
    "                        'thanks', 'advance', 'does', 'know', 'doesn', 'work', 'recovery', 'mode',\\\n",
    "                         'days', 'ago', 'lock' ,'screen', 'months', 'ago', 'power', 'button', 'old' ,'phone', \\\n",
    "                       'need', 'help', 'factory', 'reset', 'lock', 'screen'}\n",
    "stop_words = stop_words.union(additional_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production model selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                         \t| `title`         \t| `text`          \t| `comments`      \t|\n",
    "|:-----------------------:\t|-----------------\t|-----------------\t|-----------------\t|\n",
    "| Best Classifier         \t| MultinomialNB   \t| MultinomialNB   \t| MultinomialNB   \t|\n",
    "| Score on training set   \t| 0.993           \t| 0.970           \t| 0.97            \t|\n",
    "| Score on validation set \t| 0.758            \t| 0.835           \t| 0.835           \t|\n",
    "| Best vectorizer         \t| CountVectorizer \t| CountVectorizer \t| CountVectorizer \t|\n",
    "\n",
    "The above table summarises the accuracy scores on the training set and the validation set for the three sets of data. Two conclusions can be drawn from this initial model selection:\n",
    "1. Of the three kinds of content, `text` data and `comments` make the best predictors if a single set of content is available. A likely explanation for this is the sheer length of the corpus of words available in the `text` abd `comments` as compared to the `title`.\n",
    "2. The most ideal combination of classifier and vectorizer is a CountVectorizer paired with a Multinomial Naive Bayes model. This will be the combination that will be used for further hyperparameter optimisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a vectorizer and classifier for `title` content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy : 0.992790194664744\n",
      "Validation set accuracy: 0.7579831932773109\n",
      "Best Model : {'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.1, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 3), preprocessor=None,\n",
      "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
      "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
      "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
      "                            'been', 'before', 'being', 'below', 'between',\n",
      "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None), 'vectorizer__max_df': 0.1}\n",
      "CPU times: user 5.93 s, sys: 1.19 s, total: 7.12 s\n",
      "Wall time: 8.08 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    7.9s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set a pipeline to first select the individual entries, vectorize using Tfid and CountVectorize\n",
    "#and finally run classification models, \n",
    "pipeline = Pipeline([\n",
    "    ('selector', FunctionTransformer(lambda x:x['cleaned_title'])),\n",
    "    ('vectorizer', None),\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    # Cycle through 4 different classification models. \n",
    "    'classifier': [SVC(), LogisticRegression(), BernoulliNB(), MultinomialNB()],\n",
    "    # Try two different vectorizers\n",
    "    'vectorizer': [CountVectorizer(stop_words = stop_words, ngram_range = (1,3)),\n",
    "                  TfidfVectorizer(stop_words = stop_words, ngram_range = (1,3))],\n",
    "    # feature selection by max df\n",
    "    'vectorizer__max_df': [1, 0.05, 0.1]\n",
    "}]\n",
    "\n",
    "# use kfold for cv to allow shuffling\n",
    "kf = KFold(n_splits = 3, shuffle = True, random_state=42)\n",
    "\n",
    "# Gridsearch to find best models and vectorisers\n",
    "gscv_title = GridSearchCV(pipeline, cv=kf, param_grid = param_grid, scoring ='accuracy', iid=False, verbose = True, n_jobs=-1)\n",
    "gscv_title.fit(X_train, y_train)\n",
    "y_pred_title = gs_title.predict(X_test)\n",
    "\n",
    "# Scoring\n",
    "print(\"training set accuracy :\", gscv_title.score(X_train, y_train))\n",
    "print(\"Validation set accuracy:\", gscv_title.score(X_test, y_test))\n",
    "print(\"Best Model :\", gscv_title.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (title content as predictor):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted r/Androidquestions</th>\n",
       "      <th>predicted r/iphonehelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual r/Androidquestions</th>\n",
       "      <td>268</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual r/iphonehelp</th>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           predicted r/Androidquestions  \\\n",
       "actual r/Androidquestions                           268   \n",
       "actual r/iphonehelp                                  67   \n",
       "\n",
       "                           predicted r/iphonehelp  \n",
       "actual r/Androidquestions                      30  \n",
       "actual r/iphonehelp                           230  "
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "cmatrix = confusion_matrix(y_test, y_pred_title)\n",
    "print(\"Confusion matrix (title content as predictor):\")\n",
    "pd.DataFrame(cmatrix, \n",
    "             index = ['actual r/Androidquestions','actual r/iphonehelp'],\n",
    "             columns = ['predicted r/Androidquestions', 'predicted r/iphonehelp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a vectorizer and classifier for on `text` content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   11.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy : 0.969718817591925\n",
      "Validation set accuracy: 0.8352941176470589\n",
      "Best Model : {'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.1, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 3), preprocessor=None,\n",
      "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
      "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
      "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
      "                            'been', 'before', 'being', 'below', 'between',\n",
      "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None), 'vectorizer__max_df': 0.1}\n",
      "CPU times: user 8.87 s, sys: 1.79 s, total: 10.7 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('selector', FunctionTransformer(lambda x:x['cleaned_text'])),\n",
    "    ('vectorizer', None),\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    'classifier': [SVC(), LogisticRegression(), BernoulliNB(), MultinomialNB()],\n",
    "    'vectorizer': [CountVectorizer(stop_words = stop_words, ngram_range = (1,3)),\n",
    "                       TfidfVectorizer(stop_words = stop_words, ngram_range = (1,3))],\n",
    "    'vectorizer__max_df': [1, 0.05, 0.1]\n",
    "}]\n",
    "\n",
    "kf = KFold(n_splits = 3, shuffle = True, random_state=42)\n",
    "\n",
    "gscv_text = GridSearchCV(pipeline, cv=kf, param_grid = param_grid, scoring ='accuracy', iid=False, verbose = True, n_jobs=-1)\n",
    "gscv_text.fit(X_train, y_train)\n",
    "y_pred_text = gscv_text.predict(X_test)\n",
    "\n",
    "print(\"training set accuracy :\", gscv_text.score(X_train, y_train))\n",
    "print(\"Validation set accuracy:\", gscv_text.score(X_test, y_test))\n",
    "print(\"Best Model :\", gscv_text.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (title content as predictor):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted r/Androidquestions</th>\n",
       "      <th>predicted r/iphonehelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual r/Androidquestions</th>\n",
       "      <td>268</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual r/iphonehelp</th>\n",
       "      <td>68</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           predicted r/Androidquestions  \\\n",
       "actual r/Androidquestions                           268   \n",
       "actual r/iphonehelp                                  68   \n",
       "\n",
       "                           predicted r/iphonehelp  \n",
       "actual r/Androidquestions                      30  \n",
       "actual r/iphonehelp                           229  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "cmatrix = confusion_matrix(y_test, y_pred_text)\n",
    "print(\"Confusion matrix (title content as predictor):\")\n",
    "pd.DataFrame(cmatrix, \n",
    "             index = ['actual r/Androidquestions','actual r/iphonehelp'],\n",
    "             columns = ['predicted r/Androidquestions', 'predicted r/iphonehelp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a vectorizer and classifier for `comments` content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy : 0.969718817591925\n",
      "Validation set accuracy: 0.8352941176470589\n",
      "Best Model : {'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.1, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None,\n",
      "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
      "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
      "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
      "                            'been', 'before', 'being', 'below', 'between',\n",
      "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None), 'vectorizer__max_df': 0.1}\n",
      "CPU times: user 9.68 s, sys: 2.25 s, total: 11.9 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('selector', FunctionTransformer(lambda x:x['cleaned_comments'])),\n",
    "    ('vectorizer', None),\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    'classifier': [SVC(), LogisticRegression(), BernoulliNB(), MultinomialNB()],\n",
    "    'vectorizer': [CountVectorizer(stop_words = stop_words, ngram_range = (1,4)),\n",
    "                       TfidfVectorizer(stop_words = stop_words, ngram_range = (1,4))],\n",
    "    'vectorizer__max_df': [1, 0.05, 0.1]\n",
    "}]\n",
    "\n",
    "kf = KFold(n_splits = 3, shuffle = True, random_state=42)\n",
    "\n",
    "gscv_comments = GridSearchCV(pipeline, cv=kf, param_grid = param_grid, scoring ='accuracy', iid=False, verbose = True, n_jobs=-1)\n",
    "gscv_comments.fit(X_train, y_train)\n",
    "y_pred_comments = gscv_comments.predict(X_test)\n",
    "\n",
    "print(\"training set accuracy :\", gscv_comments.score(X_train, y_train))\n",
    "print(\"Validation set accuracy:\", gscv_comments.score(X_test, y_test))\n",
    "print(\"Best Model :\", gscv_comments.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (title content as predictor):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted r/Androidquestions</th>\n",
       "      <th>predicted r/iphonehelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual r/Androidquestions</th>\n",
       "      <td>268</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual r/iphonehelp</th>\n",
       "      <td>68</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           predicted r/Androidquestions  \\\n",
       "actual r/Androidquestions                           268   \n",
       "actual r/iphonehelp                                  68   \n",
       "\n",
       "                           predicted r/iphonehelp  \n",
       "actual r/Androidquestions                      30  \n",
       "actual r/iphonehelp                           229  "
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "cmatrix = confusion_matrix(y_test, y_pred_comments)\n",
    "print(\"Confusion matrix (title content as predictor):\")\n",
    "pd.DataFrame(cmatrix, \n",
    "             index = ['actual r/Androidquestions','actual r/iphonehelp'],\n",
    "             columns = ['predicted r/Androidquestions', 'predicted r/iphonehelp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of vectorizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            \t| Accuracy before  hyperparameter  optimisation \t| Accuracy after  hyperparameter  optimisation \t|\n",
    "|:----------:\t|-----------------------------------------------\t|----------------------------------------------\t|\n",
    "| `title`    \t| 0.758                                         \t| 0.763                                        \t|\n",
    "| `text`     \t| 0.835                                         \t| 0.839                                        \t|\n",
    "| `comments` \t| 0.835                                         \t| 0.839                                        \t|\n",
    "\n",
    "A second more extensive (and much longer) grid search for hyperparameter optimisation provides a very modest increase in model accuracy on the validation set (y_test) of data. Further feature engineering will be required in order to improve the accuracy scores of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for optimal parameter on `title` content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 7200 candidates, totalling 21600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9768 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11218 tasks      | elapsed: 24.5min\n",
      "[Parallel(n_jobs=-1)]: Done 12768 tasks      | elapsed: 27.9min\n",
      "[Parallel(n_jobs=-1)]: Done 14418 tasks      | elapsed: 31.8min\n",
      "[Parallel(n_jobs=-1)]: Done 16168 tasks      | elapsed: 35.6min\n",
      "[Parallel(n_jobs=-1)]: Done 18018 tasks      | elapsed: 39.4min\n",
      "[Parallel(n_jobs=-1)]: Done 19968 tasks      | elapsed: 44.1min\n",
      "[Parallel(n_jobs=-1)]: Done 21600 out of 21600 | elapsed: 47.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy : 0.9920692141312184\n",
      "Validation set accuracy: 0.7630252100840336\n",
      "Best Model : {'classifier__alpha': 1.0101010101010102, 'vectorizer__binary': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 3)}\n",
      "CPU times: user 34min 58s, sys: 8min 15s, total: 43min 13s\n",
      "Wall time: 47min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('selector', FunctionTransformer(lambda x:x['cleaned_title'])),\n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "    \n",
    "param_grid = [{\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2), (1,3), (1,4)],\n",
    "    'vectorizer__max_df': [1, 0.05, 0.1],\n",
    "    'vectorizer__min_df': [1, 0.05, 0.1],\n",
    "    'vectorizer__binary': [True, False],\n",
    "    'classifier__alpha': np.linspace(0, 100, num=100),\n",
    "   # 'classifier__binarize' : np.linspace(0, 100, num=2),\n",
    "   # 'clasifier__fit_prior' : [True,False]\n",
    "}]\n",
    "    \n",
    "kf = KFold(n_splits = 3, shuffle = True, random_state=42)\n",
    "\n",
    "gscv_title = GridSearchCV(pipeline, cv=kf, param_grid = param_grid, scoring ='accuracy', iid=False, verbose = True, n_jobs=-1)\n",
    "gscv_title.fit(X_train, y_train)\n",
    "y_pred_title = gscv_title.predict(X_test)\n",
    "\n",
    "print(\"training set accuracy :\", gscv_title.score(X_train, y_train))\n",
    "print(\"Validation set accuracy:\", gscv_title.score(X_test, y_test))\n",
    "print(\"Best Model :\", gscv_title.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (title content as predictor):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted r/Androidquestions</th>\n",
       "      <th>predicted r/iphonehelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual r/Androidquestions</th>\n",
       "      <td>236</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual r/iphonehelp</th>\n",
       "      <td>79</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           predicted r/Androidquestions  \\\n",
       "actual r/Androidquestions                           236   \n",
       "actual r/iphonehelp                                  79   \n",
       "\n",
       "                           predicted r/iphonehelp  \n",
       "actual r/Androidquestions                      62  \n",
       "actual r/iphonehelp                           218  "
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "cmatrix = confusion_matrix(y_test, y_pred_title)\n",
    "print(\"Confusion matrix (title content as predictor):\")\n",
    "pd.DataFrame(cmatrix, \n",
    "             index = ['actual r/Androidquestions','actual r/iphonehelp'],\n",
    "             columns = ['predicted r/Androidquestions', 'predicted r/iphonehelp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for optimal parameter on `text` content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3200 candidates, totalling 9600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9600 out of 9600 | elapsed: 23.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy : 0.9689978370583994\n",
      "Validation set accuracy: 0.838655462184874\n",
      "Best Model : {'classifier__alpha': 2.0202020202020203, 'vectorizer__binary': True, 'vectorizer__max_df': 0.1, 'vectorizer__ngram_range': (1, 2)}\n",
      "CPU times: user 17min 44s, sys: 3min 46s, total: 21min 31s\n",
      "Wall time: 23min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('selector', FunctionTransformer(lambda x:x['cleaned_text'])),\n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "    \n",
    "param_grid = [{\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2), (1,3), (1,4)],\n",
    "    'vectorizer__max_df': [1, 0.05, 0.1, 0.01],\n",
    "    'vectorizer__binary': [True, False],\n",
    "    'classifier__alpha': np.linspace(0, 100, num=100),\n",
    "}]\n",
    "    \n",
    "kf = KFold(n_splits = 3, shuffle = True, random_state=42)\n",
    "\n",
    "gscv_text = GridSearchCV(pipeline, cv=kf, param_grid = param_grid, scoring ='accuracy', iid=False, verbose = True, n_jobs=-1)\n",
    "gscv_text.fit(X_train, y_train)\n",
    "y_pred_text = gscv_text.predict(X_test)\n",
    "\n",
    "print(\"training set accuracy :\", gscv_text.score(X_train, y_train))\n",
    "print(\"Validation set accuracy:\", gscv_text.score(X_test, y_test))\n",
    "print(\"Best Model :\", gscv_text.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print confusion matrix\n",
    "cmatrix = confusion_matrix(y_test, y_pred_text)\n",
    "print(\"Confusion matrix (title content as predictor):\")\n",
    "pd.DataFrame(cmatrix, \n",
    "             index = ['actual r/Androidquestions','actual r/iphonehelp'],\n",
    "             columns = ['predicted r/Androidquestions', 'predicted r/iphonehelp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for optimal parameter on `comments` content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3200 candidates, totalling 9600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9600 out of 9600 | elapsed: 21.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy : 0.9689978370583994\n",
      "Validation set accuracy: 0.838655462184874\n",
      "Best Model : {'classifier__alpha': 2.0202020202020203, 'vectorizer__binary': True, 'vectorizer__max_df': 0.1, 'vectorizer__ngram_range': (1, 2)}\n",
      "CPU times: user 15min 51s, sys: 3min 17s, total: 19min 8s\n",
      "Wall time: 21min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('selector', FunctionTransformer(lambda x:x['cleaned_comments'])),\n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "    \n",
    "param_grid = [{\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2), (1,3), (1,4)],\n",
    "    'vectorizer__max_df': [1, 0.05, 0.1, 0.01],\n",
    "    'vectorizer__binary': [True, False],\n",
    "    'classifier__alpha': np.linspace(0, 100, num=100),\n",
    "}]\n",
    "    \n",
    "kf = KFold(n_splits = 3, shuffle = True, random_state=42)\n",
    "\n",
    "gscv_comments = GridSearchCV(pipeline, cv=kf, param_grid = param_grid, scoring ='accuracy', iid=False, verbose = True, n_jobs=-1)\n",
    "gscv_comments.fit(X_train, y_train)\n",
    "y_pred_comments = gscv_comments.predict(X_test)\n",
    "\n",
    "print(\"training set accuracy :\", gscv_comments.score(X_train, y_train))\n",
    "print(\"Validation set accuracy:\", gscv_comments.score(X_test, y_test))\n",
    "print(\"Best Model :\", gscv_comments.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (title content as predictor):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted r/Androidquestions</th>\n",
       "      <th>predicted r/iphonehelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual r/Androidquestions</th>\n",
       "      <td>271</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual r/iphonehelp</th>\n",
       "      <td>69</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           predicted r/Androidquestions  \\\n",
       "actual r/Androidquestions                           271   \n",
       "actual r/iphonehelp                                  69   \n",
       "\n",
       "                           predicted r/iphonehelp  \n",
       "actual r/Androidquestions                      27  \n",
       "actual r/iphonehelp                           228  "
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "cmatrix = confusion_matrix(y_test, y_pred_comments)\n",
    "print(\"Confusion matrix (title content as predictor):\")\n",
    "pd.DataFrame(cmatrix, \n",
    "             index = ['actual r/Androidquestions','actual r/iphonehelp'],\n",
    "             columns = ['predicted r/Androidquestions', 'predicted r/iphonehelp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining all the strings as predictors**\n",
    "\n",
    "The abovementioned model selection and hyperparameter optimisation grid searches, although successful in modestly improving accuracy scores, have been conducted in isolation on either `title`, `text`, or , `comment` data. `text` and `comment` data consistently perform better than `title` content hypothetically due to the sheer length of their individual corpus of words.\n",
    "\n",
    "Going by that hypothesis, combining the content from all three aspects into a single dataframe would yield a lot more data, thereby leading to a step-change improvement in the accuracy scores of the model. I hence combine `title`, `text` and `comment` data into a single dataframe called `combined_content`, perform a `train_test_split` in order to achieve training data with a lot more data points. Using the `combined_data`, I perform another gridsearch for opptimised hyperparameters. The result accuracy of this method yields an accuracy score of **0.899**, a significant imporvement over the accuracy scores achieved by hyperparameter optimisation alone.\n",
    "\n",
    "This feature engineering method of combining all `title`, `text` and `comment` data into a single dataframe before running it through a Multinomial Naive Bayes model with optimised hyperparameters will be used on the test data for a final accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch for optimal parameters on combined content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with all the content combined as a single column\n",
    "df1 = df[['cleaned_title', 'subreddit']]\n",
    "df1.rename({'cleaned_title': 'content'}, axis=1, inplace=True)\n",
    "\n",
    "df2 = df[['cleaned_text', 'subreddit']]\n",
    "df2.rename({'cleaned_text': 'content'}, axis=1, inplace=True)\n",
    "\n",
    "df3 = df[['cleaned_comments', 'subreddit']]\n",
    "df3.rename({'cleaned_comments': 'content'}, axis=1, inplace=True)\n",
    "\n",
    "combined_content = pd.concat([df1,df2,df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4162, 1)\n",
      "(4162,)\n",
      "(1784, 1)\n",
      "(1784,)\n"
     ]
    }
   ],
   "source": [
    "X = combined_content.drop('subreddit', axis=1)\n",
    "y = combined_content['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                              y, \n",
    "                                              test_size = 0.3, \n",
    "                                              random_state = 42, \n",
    "                                              stratify = y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3200 candidates, totalling 9600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9600 out of 9600 | elapsed: 14.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy : 0.9723690533397406\n",
      "Validation set accuracy: 0.898542600896861\n",
      "Best Model : {'classifier__alpha': 1.0101010101010102, 'vectorizer__binary': True, 'vectorizer__max_df': 0.1, 'vectorizer__ngram_range': (1, 3)}\n",
      "CPU times: user 2min 4s, sys: 1min 20s, total: 3min 25s\n",
      "Wall time: 14min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('selector', FunctionTransformer(lambda x:x['content'])),\n",
    "    ('vectorizer', CountVectorizer(stop_words = stop_words)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "    \n",
    "param_grid = [{\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2), (1,3), (1,4)],\n",
    "    'vectorizer__max_df': [1, 0.05, 0.1, 0.01],\n",
    "    'vectorizer__binary': [True, False],\n",
    "    'classifier__alpha': np.linspace(0, 100, num=100),\n",
    "}]\n",
    "    \n",
    "kf = KFold(n_splits = 3, shuffle = True, random_state=42)\n",
    "\n",
    "gscv_combined = GridSearchCV(pipeline, cv=kf, param_grid = param_grid, scoring ='accuracy', iid=False, verbose = True, n_jobs=-1)\n",
    "gscv_combined.fit(X_train, y_train)\n",
    "y_pred_combined = gscv_combined.predict(X_test)\n",
    "\n",
    "print(\"training set accuracy :\", gscv_combined.score(X_train, y_train))\n",
    "print(\"Validation set accuracy:\", gscv_combined.score(X_test, y_test))\n",
    "print(\"Best Model :\", gscv_combined.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (All content as predictor):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted r/Androidquestions</th>\n",
       "      <th>predicted r/iphonehelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual r/Androidquestions</th>\n",
       "      <td>804</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual r/iphonehelp</th>\n",
       "      <td>92</td>\n",
       "      <td>799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           predicted r/Androidquestions  \\\n",
       "actual r/Androidquestions                           804   \n",
       "actual r/iphonehelp                                  92   \n",
       "\n",
       "                           predicted r/iphonehelp  \n",
       "actual r/Androidquestions                      89  \n",
       "actual r/iphonehelp                           799  "
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "cmatrix = confusion_matrix(y_test, y_pred_combined)\n",
    "print(\"Confusion matrix (All content as predictor):\")\n",
    "pd.DataFrame(cmatrix, \n",
    "             index = ['actual r/Androidquestions','actual r/iphonehelp'],\n",
    "             columns = ['predicted r/Androidquestions', 'predicted r/iphonehelp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/combined_test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 21)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of test data\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "id               0\n",
       "date_created     0\n",
       "text             0\n",
       "score            0\n",
       "upvote_ratio     0\n",
       "comment_count    0\n",
       "comment_all      0\n",
       "comment_len      0\n",
       "title_len        0\n",
       "text_len         0\n",
       "subreddit        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise\n",
    "test['tok_title'] = test.apply(lambda row: nltk.word_tokenize(row['title'].lower()), axis=1)\n",
    "test['tok_text'] = test.apply(lambda row: nltk.word_tokenize(row['text'].lower()), axis=1)\n",
    "test['tok_comments'] = test.apply(lambda row: nltk.word_tokenize(row['comment_all'].lower()), axis=1)\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "test['lem_tok_title'] = test['tok_title'].apply(lambda row: [lemmatizer.lemmatize(item) for item in row])\n",
    "test['lem_tok_text'] = test['tok_text'].apply(lambda row: [lemmatizer.lemmatize(item) for item in row])\n",
    "test['lem_tok_comments'] = test['tok_comments'].apply(lambda row: [lemmatizer.lemmatize(item) for item in row])\n",
    "\n",
    "# Join\n",
    "test[\"cleaned_title\"]= test[\"lem_tok_title\"].str.join(\" \")\n",
    "test[\"cleaned_text\"]= test[\"lem_tok_text\"].str.join(\" \")\n",
    "test[\"cleaned_comments\"]= test[\"lem_tok_text\"].str.join(\" \")\n",
    "\n",
    "#remove punctuations\n",
    "test.cleaned_title = test.cleaned_title.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "test.cleaned_text = test.cleaned_text.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "test.cleaned_comments = test.cleaned_comments.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with all the content combined as a single column\n",
    "test1 = test[['cleaned_title', 'subreddit']]\n",
    "test1.rename({'cleaned_title': 'content'}, axis=1, inplace=True)\n",
    "\n",
    "test2 = test[['cleaned_text', 'subreddit']]\n",
    "test2.rename({'cleaned_text': 'content'}, axis=1, inplace=True)\n",
    "\n",
    "test3 = test[['cleaned_comments', 'subreddit']]\n",
    "test3.rename({'cleaned_comments': 'content'}, axis=1, inplace=True)\n",
    "\n",
    "combined_test = pd.concat([test1,test2,test3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and y on test data\n",
    "X = combined_test.drop('subreddit', axis=1)\n",
    "y = combined_test['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy score on test data: 0.83\n",
      "Confusion matrix (On unlabelled test data):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted r/Androidquestions</th>\n",
       "      <th>predicted r/iphonehelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual r/Androidquestions</th>\n",
       "      <td>299</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual r/iphonehelp</th>\n",
       "      <td>172</td>\n",
       "      <td>791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           predicted r/Androidquestions  \\\n",
       "actual r/Androidquestions                           299   \n",
       "actual r/iphonehelp                                 172   \n",
       "\n",
       "                           predicted r/iphonehelp  \n",
       "actual r/Androidquestions                      58  \n",
       "actual r/iphonehelp                           791  "
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = gscv_combined.predict(X)\n",
    "\n",
    "print(f\"Final accuracy score on test data: {np.round(accuracy_score(y, y_pred_test), 2)}\")\n",
    "\n",
    "# print confusion matrix\n",
    "cmatrix = confusion_matrix(y, y_pred_test)\n",
    "print(\"Confusion matrix (On unlabelled test data):\")\n",
    "pd.DataFrame(cmatrix, \n",
    "             index = ['actual r/Androidquestions','actual r/iphonehelp'],\n",
    "             columns = ['predicted r/Androidquestions', 'predicted r/iphonehelp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a multinomial naive Bayes classifier trained on a combination of title, post and comment content, I was able to classify **440** unlabelled posts into r/Androidquestions or r/iphonehelp with a reasonable accuracy of **83%**. This far outperformed the baseline score of *50%*.\n",
    "\n",
    "The nature of queries in r/Androidquestions appear mostly related to software issues and tweaking issues related to the Android 10 operating system as opposed to the hardware. In contrast, most issues on r/iphonehelp are related to hardware problems such as accidentily dropping phones in water or replacing cracked screens. It is perhaps unsurprising then that it is easy to distinguish between posts meant for either subreddits. Keywords such as 'Android' and 'ios' which are native to the different operating systems further help the discrimation of the posts.\n",
    "\n",
    "Despite the differences in the different phone ecosystems, they still share some similar issues, which is a likely explanation for the model misclassifications. Looking at overlapping words ('sim card', 'factory reset', 'recovery mode', 'lock screen', 'power button', 'old phone') between the top 50 meaningful phrases gives us an indication of the common issues which most likely stratify both subreddits.\n",
    "\n",
    "To further improve model accuracy, a bigger corpus that incorporates a bigger vocabulary on the different systems is needed. As proven through the data on modelling, models trained using only title information tend to be more inaccurate as compared to the text and comment data which tended to be longer in nature, hence containing more words. The best model on the validation set incorporated *4,162* data points which were a combination of title, comment and text data. In contrast, hyperparameter optimisation, though time-consuming, only achieved very modest accuracy gains.\n",
    "It can hence be said that the hypothesis of 'throw more data at the model' to improve accuracy scores holds true. The model does not discriminate between title, text and comments but merely the vocabulary of words within an entire subreddit post. I hence posit, that if this model were deployed for real-life use, the substantial increase in queries and discriptions of problems over time would improve accuracy scores. \n",
    "\n",
    "To move the project forward (i.e. to improve accuracy scores) I recommend the following:\n",
    "1. Feed all 'text'-related information as a single feature into the model\n",
    "1. Deploy the model and put it in use so as to 'crowd-source' a larger corpus of words as queries come in.\n",
    "1. Use other sources of data such as other subreddits and other forums to increase th ecorpus of words.\n",
    "\n",
    "As mentioned previously, although the goal of this project is to classify subreddits, such a classification model can also be applied elsewhere, such as to automate front end systems for topic matching and routing of queries to the right troubleshooting teams, recommending possible solutions as part of a larger software system, and the ever-useful spam filtering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
